{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2974ce-76af-4d88-998d-ea8df948c71f",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ddd33-d346-40da-bb09-488e20d45415",
   "metadata": {},
   "source": [
    "La tokenisation est le processus de segmentation d’un texte en unités (tokens) significatives (phrases, mots, symboles…). Elle constitue souvent la première étape avant toute analyse de texte.\n",
    "\n",
    "Commençons par charger la bibliothèque spaCy qui facilite les opérations de segmentation, ainsi que deux textes en anglais et en français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55643092-efca-4822-ac7b-367f76ce844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en = \"Little Joe sat down on the bank and prepared to enjoy his breakfast. He hadn't seen Buster Bear, and he didn't know that he or any one else was anywhere near.\"\n",
    "fr = \"Elvire, m’as-tu fait un rapport bien sincère ? Ne déguises-tu rien de ce qu’a dit mon père ?\"\n",
    "\n",
    "nlp_en = spacy.blank(\"en\")\n",
    "nlp_fr = spacy.blank(\"fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a449236-8b7d-497a-af93-457de5b0725b",
   "metadata": {},
   "source": [
    "## Segmentation en phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e124d46-48e5-4747-a121-01f80b089559",
   "metadata": {},
   "source": [
    "La segmentation en phrases n’est pas disponible par défaut dans les modèles de langage de spaCy. La méthode `.add_pipe()` permet de charger la ressource nécessaire dans le pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ed8ef-a260-4064-a4c5-4813bf28cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sentencizer to the pipeline\n",
    "nlp_en.add_pipe('sentencizer')\n",
    "nlp_fr.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1849235-ee0d-4cee-aa5e-82f5b3f7e2d1",
   "metadata": {},
   "source": [
    "La liste des composants installés dans un pipeline est ensuite accessible via une liste stockée dans un attribut `.components` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f272c4d-7304-4b4c-8365-781902e2a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en.components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8dc3c-7282-4235-bea9-52dc8f1b77a6",
   "metadata": {},
   "source": [
    "Nous pouvons ensuite passer les textes dans les pipelines :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e39a6-756d-4892-8e64-827f237d1083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the documents\n",
    "doc_en = nlp_en(en)\n",
    "doc_fr = nlp_fr(fr)\n",
    "\n",
    "print(\n",
    "    [ sent.text for sent in doc_en.sents ],\n",
    "    [ sent.text for sent in doc_fr.sents ],\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570fe5f-aabc-4349-bcab-c53264b4ecb3",
   "metadata": {},
   "source": [
    "## Segmentation en mots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4d780-a3a1-49ce-a019-d4bc6bd92bc8",
   "metadata": {},
   "source": [
    "Cette opération est inclue nativement dans les pipelines de spaCy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830eb1a-ff29-4c3b-ba2d-c6fdcbcaecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_en:\n",
    "    print(token.text, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2bdcf1-9707-4e25-af8b-8054b6246d47",
   "metadata": {},
   "source": [
    "Parmi les choix notables, elle conserve les signes de ponctuation et sépare les formes contractées : *hadn't* devient *had* et *n't*. Regardons si les choix sont identiques pour le français :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ba0fe-ccb9-44bc-b591-c69cd2f0705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_fr:\n",
    "    print(token.text, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082f107-a325-4030-b030-4f2d6528afa8",
   "metadata": {},
   "source": [
    "La segmentation est conforme à ce que l’on voudrait pour du français : *m'as-tu* a bien été divisé en quatre tokens avec, pour l’apostrophe accrochée au premier d’entre eux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8e4ed-0567-4ac4-a190-e4c32d109fab",
   "metadata": {},
   "source": [
    "### Une segmentation sur les caractères blancs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653d6e0-f8a7-41cc-9d26-2b72802bccd8",
   "metadata": {},
   "source": [
    "Comment personnaliser le tokenisateur ? Si l’objectif est de conserver les contractions de l’anglais ou les élisions du français, il est possible de programmer facilement un composant qui fasse la séparation sur les caractères blancs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80616dca-47e3-4f14-b93c-dd693295bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer_en = Tokenizer(nlp_en.vocab)\n",
    "tokenizer_fr = Tokenizer(nlp_fr.vocab)\n",
    "\n",
    "print(\n",
    "    tokenizer_en(en),\n",
    "    tokenizer_fr(fr),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7315a59-d29a-4334-8760-70f923b701d4",
   "metadata": {},
   "source": [
    "L’idée est d’entraîner une instance de la classe `Tokenizer` à l’aide du seul vocabulaire du modèle de langage concerné. Ces vocabulaires contiennent des informations statistiques sur les mots-formes des langues traitées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d7a5a-425f-49cc-ae34-166d756f7f18",
   "metadata": {},
   "source": [
    "### Ajouter des règles spéciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c7cb2-1355-41d8-92ca-69dcb0334c9f",
   "metadata": {},
   "source": [
    "La plupart du temps, il sera plutôt question d’agir sur le tokenisateur par défaut en lui ajoutant des règles spéciales. Mais avant, cela implique de bien comprendre les étapes de la tokenisation :\n",
    "\n",
    "![Étapes de la tokenisation](./images/tokenization.svg)\n",
    "\n",
    "Le schéma peut se comprendre ainsi :\n",
    "\n",
    "1. Segmenter grâce aux espaces ;\n",
    "2. vérifier si une règle spéciale s’applique à chaque sous-chaîne ;\n",
    "3. rechercher une correspondance avec un token ;\n",
    "4. autrement rechercher un préfixe ;\n",
    "5. en l’absence de préfixe, rechercher un suffixe ;\n",
    "6. en l’absence de suffixe, vérifier à nouveu si une règle spéciale ne s’applique pas.\n",
    "\n",
    "À ces étapes, il faudrait aussi appliquer quelques autres règles, comme la recherche des infixes (traits d’unions…) et des motifs d’URL.\n",
    "\n",
    "Regardons une phrase en français qui ne devrait guère poser de difficultés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f70af-706f-4544-85dc-23ed586fdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_fr(\"Y a-t-il un médecin dans la salle ?\")\n",
    "for t in doc:\n",
    "    print(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a02b46-c03c-42cb-ad72-93432147ca97",
   "metadata": {},
   "source": [
    "Lorsque, plus tôt, *spaCy* a bien analysé l’inversion du sujet avec la structure *m'as-tu*, il échoue à la repérer ici. Pour preuve, le trait d’union reste attaché aux tokens *-t* et *-il*.\n",
    "\n",
    "Pour remédier à ce défaut, ajoutons une règle spéciale au tokenisateur grâce à la méthode `.add_special_case()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb9623-c039-4929-b755-94c1a75ade2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "special_case = [{ORTH: \"a\"}, {ORTH: \"-\"}, {ORTH: \"t\"}, {ORTH: \"-\"}, {ORTH: \"il\"}]\n",
    "\n",
    "nlp_fr.tokenizer.add_special_case(\"a-t-il\", special_case)\n",
    "\n",
    "doc = nlp_fr(\"Y a-t-il un médecin dans la salle ?\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce38fb-ddf5-49ac-8af5-4a8c36701b0f",
   "metadata": {},
   "source": [
    "## Filtrer une liste de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66462552-2c3e-4377-ab6f-821ee4bc7f18",
   "metadata": {},
   "source": [
    "Après la phase de segmentation, il arrive souvent que l’on ne veuille pas conserver tous les tokens identifiés dans un texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e7876-450d-4de4-b46a-913bc7030fb2",
   "metadata": {},
   "source": [
    "### Supprimer les mots vides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf46db-1a0b-48cf-b1ca-b172252ba202",
   "metadata": {},
   "source": [
    "L‘une des opérations les plus courantes consiste à filtrer les mots vides d’une langue pour ne retenir que ceux signifiants. Chaque modèle de langage dispose d’une propriété `.stop_words` pour les lister :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476104e-e0b9-413c-b355-875cefed3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_fr = spacy.lang.fr.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e35481-6ede-4f94-a236-584aa9bf938c",
   "metadata": {},
   "source": [
    "Le filtrage peut s’effectuer de manière procédurale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d46cda3-5461-42cc-bef9-d4a38750595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [\n",
    "    token\n",
    "    for token in doc_fr\n",
    "    if token.text.lower() not in stopwords_fr\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835e14ab-69f8-4080-af71-3dbe19b8c32b",
   "metadata": {},
   "source": [
    "Ou, mieux, grâce à la fonction `filter()` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a0961e-8e4a-4b18-9596-3ba02860f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter(lambda token: token.text.lower() not in stopwords_fr, doc_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd250b19-3fd7-4791-8571-15f38883b4e4",
   "metadata": {},
   "source": [
    "Observons le résultat du traitement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402b241-d57f-4078-8b98-453a123c7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in filtered:\n",
    "    print(token.text, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100baf7-a516-4577-8e23-83a70f972913",
   "metadata": {},
   "source": [
    "### Supprimer les signes de ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a146ce-aaa3-409b-a9bc-f557eeb26d33",
   "metadata": {},
   "source": [
    "En soi, il n’est pas nécessaire avec *spaCy* de supprimer les signes de ponctuation. L’attribut lexical `.is_punct` permet de les exclure d’un traitement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7fafd-76fb-49dd-8e44-e9cfe0976346",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc_fr:\n",
    "    if not token.is_punct:\n",
    "        print(token.text, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ca6e3-d029-40e0-bf81-d0e977f3e65f",
   "metadata": {},
   "source": [
    "Autrement, il est facile de l’intégrer au filtre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627c79f-cdaa-404b-bdac-5e37bebf0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter(lambda token: token.text.lower() not in stopwords_fr and not token.is_punct, doc_fr)\n",
    "for token in filtered:\n",
    "    print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1156e-fd8b-4ba1-9d25-23b0921dd100",
   "metadata": {},
   "source": [
    "### Autres traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd69b42-d0c1-42ca-b909-98d6319e4fef",
   "metadata": {},
   "source": [
    "Pour rappel, il existe deux autres attributs lexicaux qui peuvent être utilisés afin de filtrer les tokens :\n",
    "\n",
    "- `.is_alpha` : le token est-il constitué de caractères alphabétiques ?\n",
    "- `.is_space` : le token fait-il partie de la familles des caractères blancs ?\n",
    "- `.like_num` : le token ressemble-t-il à un nombre ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de14765-ae42-481f-ba56-6eb9a9643832",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp_fr(\"En 1492, Christophe Colomb a semble-t-il découvert l’Amérique.\"):\n",
    "    if not token.like_num and not token.is_punct:\n",
    "        print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
